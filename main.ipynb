{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distribución de la variable objetivo es:\n",
      " 0    3099\n",
      "1     557\n",
      "Name: TenYearCHD, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.drop(['education'], axis=1)\n",
    "dataset =None\n",
    "\n",
    "\n",
    "\n",
    "print(\"La distribución de la variable objetivo es:\\n\",df['TenYearCHD'].value_counts())\n",
    "X = df.drop(['TenYearCHD'], axis=1).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna sex no parece provenir de una distribución normal\n",
      "La columna age no parece provenir de una distribución normal\n",
      "La columna currentSmoker no parece provenir de una distribución normal\n",
      "La columna cigsPerDay no parece provenir de una distribución normal\n",
      "La columna BPMeds no parece provenir de una distribución normal\n",
      "La columna prevalentStroke no parece provenir de una distribución normal\n",
      "\n",
      "Las varaibles significativas ['sex', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds', 'prevalentStroke']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "\n",
    "\n",
    "variables_significativas =[]\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    stat, p = shapiro(X[:,i])\n",
    "    alpha = 0.05\n",
    "    variables_significativas.append(df.columns[i])\n",
    "    if p > alpha:\n",
    "        print(f'La columna {df.columns[i]} parece provenir de una distribución normal')\n",
    "    else:\n",
    "        print(f'La columna {df.columns[i]} no parece provenir de una distribución normal')\n",
    "        \n",
    "print(\"\\nLas varaibles significativas\",variables_significativas)\n",
    "variables_significativas.append('TenYearCHD')\n",
    "\n",
    "dataset = df[variables_significativas]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nueva distribución de la variable objetivo es:\n",
      " 0    3099\n",
      "1    3099\n",
      "Name: TenYearCHD, dtype: int64\n",
      "\n",
      "Precisión del modelo con un grado 2 es: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "y = dataset['TenYearCHD']\n",
    "x = dataset.drop(['TenYearCHD'], axis=1)\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=1234)\n",
    "x,y = sm.fit_resample(x, y)\n",
    "print(\"La nueva distribución de la variable objetivo es:\\n\",y.value_counts())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=500, random_state=1234)\n",
    "\n",
    "logreg.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_poly)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nPrecisión del modelo con un grado 2 es: {accuracy:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor grado de polinomio: 2\n",
      "Precisión de la validación cruzada con el mejor grado de polinomio: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros\n",
    "param_grid = {'poly__degree': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=500,random_state=1234)\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('logreg', model)\n",
    "])\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f'Mejor grado de polinomio: {grid_search.best_params_[\"poly__degree\"]}')\n",
    "print(f'Precisión de la validación cruzada con el mejor grado de polinomio: {grid_search.best_score_:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando nuevamente el modelo pero utilizando los valores obtenidos en el paso anterior, se obtiene el siguiente resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisión del modelo con un grado 2 es: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree= grid_search.best_params_[\"poly__degree\"])\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=500, random_state=1234)\n",
    "\n",
    "logreg.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_poly)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nPrecisión del modelo con un grado {grid_search.best_params_[\"poly__degree\"]} es: {accuracy:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1.5\n",
    "\n",
    "Utilizando PCA con 6 componentes se obtuvieron que las features más importantes son:\n",
    "sex, age, currentSmoker, cigsPerDay, BPMeds, prevalentStroke. Si utilizamos más componentes la metrica de desempeño empeora. Education fue retirada desde el inicio ya que la educación no es un factor que influya en la probabilidad de tener un infarto de forma directa.\n",
    "\n",
    "La primera vez que realice el modelo se me olvido verificar si el dataset estaba balanceado lo que me llevo a tener overfiting. Para dicha iteración el modelo obtuvo un accuracy de 0.89 ya que era muy bueno para predecir cuando el paciente no tuvo infarto.Como el dataset no estaba balanceado opte por utilizar SMOTE para generar datos y asi balancear el dataset. Una vez balanceado repeti el proceso y el modelo con grado polinomial 2 obtuvo un accuracy de 0.65. Luego mediante GridSearchCV se obtuvo que el mejor modelo es con grado polinomial 2 y con un accuracy de 0.65 igual en mi primer intento. Como conclusión se puede decir que el modelo no es bueno para predecir cuando un paciente va a tener un infarto, a pesar de que en el analisis inicial las variables parcian ser muy significativas respecto a la varaible obejtivo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
